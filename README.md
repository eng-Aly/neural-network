

# Key Learnings:
## Epochs:
 Each epoch is one full pass through the training data, updating the model to improve accuracy over multiple epochs.

## Weights:
 Parameters that transform inputs as they pass through the network, adjusting based on errors during training.

## Forward Propagation:
 Passes inputs through layers using weights and activation functions like Sigmoid to calculate the output.

## Back propagation:
 Adjusts weights by minimizing errors using the gradient of the cost function via Stochastic Gradient Descent.

## Training Process:
 The network improves through multiple epochs, refining weights after each mini-batch.
